{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Acquire\n",
    "\n",
    "1st step is to aquire the data from the Codeup db."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Libraries\n",
    "\n",
    "# ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Wrangling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# preparing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, explained_variance_score\n",
    "\n",
    "# modeling and evaluating\n",
    "from sklearn.linear_model import LinearRegression, LassoLars\n",
    "from sklearn.linear_model import TweedieRegressor\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.preprocessing import MinMaxScaler, RobustScaler, StandardScaler\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "# Exploring\n",
    "import scipy.stats as stats\n",
    "import math\n",
    "\n",
    "# Visualizing\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# default pandas decimal number display format\n",
    "# pd.options.display.float_format = '{:20,.2f}'.format\n",
    "\n",
    "# import acquire\n",
    "# import summarize\n",
    "# import prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import acquire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add how handling nulls and document how we're handling them.\n",
    "# Handle outliers and document (add to README)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing and displaying the zillow dataframe\n",
    "\n",
    "df = acquire.get_zillow_data()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding the shape of the dataframe to make sure it matches the shape of the data that I built in MySQLPro.\n",
    "\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using .info() to find the dtypes of the columns, and to have a baseline count of non-nulls in each column to compare to my nulls when I get into the prep stage.\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.taxamount.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I think a better way of dividing up these variables might be using cut??\n",
    "def roundup(x):\n",
    "    return int(math.ceil(x / 100.0)) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rounddown(x):\n",
    "    return int(math.floor(x / 100.0)) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Write a function that takes in a dataframe of observations and attributes and returns a dataframe where each row is an atttribute name, the first column is the number of rows with missing values for that attribute, and the second column is percent of total rows that have missing values for that attribute. Run the function and document takeaways from this on how you want to handle missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_row_missing = df.isna().sum()\n",
    "num_row_missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decided to round this column, but can remove the round() command if needed to match the curriculum exactly.\n",
    "pct_rows_missing = round(num_row_missing/df.shape[0], 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_missing = pd.DataFrame({'num_row_missing': num_row_missing, 'pct_rows_missing': pct_rows_missing})\n",
    "df_missing.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now to create the function:\n",
    "\n",
    "def zillow_missing_values(df):\n",
    "    \n",
    "    total_rows = df.shape[0]\n",
    "    \n",
    "    # Count of missing values per column\n",
    "    num_row_missing = df.isna().sum()\n",
    "    \n",
    "    # Pct of missing values per column\n",
    "    pct_rows_missing = num_row_missing/total_rows\n",
    "    \n",
    "    df_missing = pd.DataFrame({'num_row_missing': num_row_missing, 'pct_rows_missing': pct_rows_missing})\n",
    "    \n",
    "    return df_missing\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_missing_test = zillow_missing_values(df)\n",
    "df_missing_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Takeaways on Missing Values\n",
    "\n",
    "- There are three points about taking care of the missing values:\n",
    "    1. Columns at > 50% missing values should probably be dropped.\n",
    "    2. Or if there is no need for the column from a modeling perspective, and most of the column is missing values, then it's dropped.\n",
    "    3. Conversely, if a column has > 50% missing values but there is a modeling need for that data, the column may be retained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Columns Missing Info\n",
    "\n",
    "4. Write a function that takes in a dataframe and returns a dataframe with 3 columns: the number of columns missing, percent of columns missing, and number of rows with n columns missing. Run the function and document takeaways from this on how you want to handle missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Percentage of each row that have missing column info.\n",
    "df.isna().sum(axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Percentage of each row that do not have missing column info.\n",
    "\n",
    "# df.notna().sum(axis = 1) / df.shape[1]\n",
    "df.isna().sum(axis = 1) / df.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# of rows missing\n",
    "\n",
    "df.isna().sum(axis = 1).value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pct cols missing\n",
    "(df.isna().sum(axis = 1).value_counts(normalize = True).sort_index() * 100).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trying to reset the column so that I can get the dataframe above...\n",
    "\n",
    "df_col = pd.DataFrame({'pct_cols_missing': (df.isna().sum(axis = 1).value_counts(normalize = True).sort_index() * 100).reset_index()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_col['num_cols_missing'] = df.isna().sum(axis = 1)\n",
    "df_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This isn't right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_and_percent_missing_column(df):\n",
    "    num_rows = df.loc[:].isnull().sum()\n",
    "    num_cols_missing = df.loc[:, df.isna().any()].count()\n",
    "    pct_cols_missing = round(df.loc[:, df.isna().any()].count() / len(df.index) * 100, 3)\n",
    "    missing_cols_and_rows_df = pd.DataFrame({'num_cols_missing': num_cols_missing,\n",
    "                                             'pct_cols_missing': pct_cols_missing,\n",
    "                                             'num_rows': num_rows})\n",
    "    missing_cols_and_rows_df = missing_cols_and_rows_df.fillna(0)\n",
    "    missing_cols_and_rows_df['num_cols_missing'] = missing_cols_and_rows_df['num_cols_missing'].astype(int)\n",
    "    return missing_cols_and_rows_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2_test = count_and_percent_missing_column(df)\n",
    "df2_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['num_cols_missing'] = df.isna().sum(axis = 1)\n",
    "df.num_cols_missing.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['pct_cols_missing'] = df.isna().sum(axis = 1) / df.shape[1]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "round(df.loc[:, df.isna().any()].count() / len(df.index) * 100, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[:, df.isna().any()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum(axis = 1) / df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I still think I should be able to use what I've already done above with value_counts to complete this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull()/df.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_col = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_missing = df.isnull().sum()\n",
    "rows_count = df.shape[0]\n",
    "pct_missing = num_missing / rows_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_missing = pd.DataFrame({'number_missing_rows': num_missing, 'pct_rows_missing': pct_missing})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ryan's solutions:\n",
    "\n",
    "def nulls_by_col(df):\n",
    "    num_missing = df.isnull().sum()\n",
    "    rows_count = df.shape[0]\n",
    "    pct_missing = num_missing / rows_count\n",
    "    cols_missing = pd.DataFrame({'number_missing_rows': num_missing, 'pct_rows_missing': pct_missing})\n",
    "    return cols_missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nulls_by_col(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary; I need to better understand how the [] and [[]] work together to creating iteration and when they do not create iteration.\n",
    "\n",
    "# I still think there's a way to use value_counts() to get this done, but I'm running into problems with the %\n",
    "\n",
    "# So I was close; Ryan in his answers kinda used that concept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attempting to build dataframe only using .value_counts to group.\n",
    "# (df.isna().sum(axis = 1).value_counts(normalize = True).sort_index() * 100).reset_index()\n",
    "\n",
    "df3_test = pd.DataFrame(df.isna().sum(axis = 1).value_counts().sort_index().reset_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Ryan's solution:\n",
    "\n",
    "def nulls_by_row(df):\n",
    "    num_cols_missing = df.isnull().sum(axis=1)\n",
    "    pct_cols_missing = df.isnull().sum(axis=1)/df.shape[1]*100\n",
    "    rows_missing = pd.DataFrame({'num_cols_missing': num_cols_missing, 'pct_cols_missing': pct_cols_missing}).reset_index().groupby(['num_cols_missing','pct_cols_missing']).count().rename(index=str, columns={'index': 'num_rows'}).reset_index()\n",
    "    return rows_missing \n",
    "\n",
    "null_rows = nulls_by_row(df)\n",
    "null_rows.sort_values(by=\"pct_cols_missing\", ascending=False, inplace=True)\n",
    "null_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nice. So the key here is that I use the .value_counts AFTER we create the function. Not inside the function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Remove any properties that are likely to be something other than single unit properties. (e.g. no duplexes, no land/lot, ...). There are multiple ways to estimate that a property is a single unit, and there is not a single \"right\" answer. But for this exercise, do not purely filter by unitcnt as we did previously. Add some new logic that will reduce the number of properties that are falsely removed. You might want to use # bedrooms, square feet, unit type or the like to then identify those with unitcnt not defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.propertylandusedesc.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plan for Isolating Single Use properties:\n",
    "\n",
    "Since the vast majority of all single use properties appear to be single use homes, I'm going to use these codes to isolate the properties that I want: 261.0, 260.0, 262.0, 263.0, 264.0. That way I'm not only using `unitcnt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking at the type of building and remove those that aren't highly likely to be single unit properties...\n",
    "\n",
    "include_ids = [261.0, 260.0, 262.0, 263.0, 264.0]\n",
    "df = df[df.propertylandusetypeid.isin(include_ids)]\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.unitcnt.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2:\n",
    "\n",
    "Create a function that will drop rows or columns based on the percent of values that are missing: handle_missing_values(df, prop_required_column, prop_required_row).\n",
    "\n",
    "- The input:\n",
    "    - A dataframe\n",
    "    - A number between 0 and 1 that represents the proportion, for each column, of rows with non-missing values required to keep the column. i.e. if prop_required_column = .6, then you are requiring a column to have at least 60% of values not-NA (no more than 40% missing).\n",
    "    - A number between 0 and 1 that represents the proportion, for each row, of columns/variables with non-missing values required to keep the row. For example, if prop_required_row = .75, then you are requiring a row to have at least 75% of variables with a non-missing value (no more that 25% missing).\n",
    "- The output:\n",
    "    - The dataframe with the columns and rows dropped as indicated. *Be sure to drop the columns prior to the rows in your function.*\n",
    "- **Hint:**\n",
    "    - Look up the dropna documentation.\n",
    "    - You will want to compute a threshold from your input values (prop_required) and total number of rows or columns.\n",
    "    - Make use of inplace, i.e. inplace=True/False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping the unecessary columns:\n",
    "\n",
    "df = df.drop(columns = ['id.1', 'id'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_id_cols(df):\n",
    "    df = df.drop(columns = ['id.1', 'id'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = int(round(.6*len(df.index),0))\n",
    "threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the property function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zillow = acquire.get_zillow_data()\n",
    "zillow.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_list_choice = [261.0, 260.0, 262.0, 263.0, 264.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def property_type_focus(df, id_list):\n",
    "#     id_list = [261.0, 260.0, 262.0, 263.0, 264.0]\n",
    "    df = df[df.propertylandusetypeid.isin(id_list)]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zillow.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zillow = property_type_focus(zillow, id_list_choice)\n",
    "zillow.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_missing_values(df, col_limit = .6, row_limit = .6):\n",
    "    \n",
    "    df.drop(columns = ['id.1', 'id', 'propertyzoningdesc', 'calculatedbathnbr'], inplace = True)\n",
    "    df.drop\n",
    "    # Setting the threshold for columns to drop:\n",
    "    col_thresh = int(round(col_limit * len(df.index), 0))\n",
    "    df.dropna(axis = 1, thresh = col_thresh, inplace = True)\n",
    "    # Now for the rows:\n",
    "    row_thresh = int(round(col_limit * len(df.columns), 0))\n",
    "    df.dropna(axis = 0, thresh = row_thresh, inplace = True)\n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zillow = handle_missing_values(zillow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zillow.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting Zillow Data Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_zillow_data(df):\n",
    "    # df = get_mall_data()\n",
    "    # Splitting my data based on the target variable of tenure:\n",
    "    train_validate, test = train_test_split(df, test_size=.15, random_state=123)\n",
    "    \n",
    "    # Splitting the train_validate set into the separate train and validate datasets.\n",
    "    train, validate = train_test_split(train_validate, test_size=.20, random_state=123)\n",
    "    \n",
    "    # Printing the shape of each dataframe:\n",
    "    print(f'Shape of train df: {train.shape}')\n",
    "    print(f'Shape of validate df: {validate.shape}')\n",
    "    print(f'Shape of test df: {test.shape}')\n",
    "    return train, validate, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, validate, test = split_zillow_data(zillow)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.isnull().sum().sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Filling Missing Values\n",
    "\n",
    "**After splitting the df into train, validate, test:**\n",
    "\n",
    "Decide how to handle the remaining missing values:\n",
    "\n",
    "- Fill with constant value.\n",
    "- Impute with mean, median, mode.\n",
    "- Drop row/column\n",
    "\n",
    "In deciding how to fill the missing values, I'm going to for the mean for the majority of the columns. Knowing the data and what I'm trying to find from it, adding a bunch of values to the average *shouldn't* change the outcome of any model I'm building too much. However, I'll have to bookmark the decision I'm making here and come back to it (and maybe change my mind) if the models I start creating are appears to be messed up in some way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute missing values, after splitting.\n",
    "\n",
    "cols = [\n",
    "    \"structuretaxvaluedollarcnt\",\n",
    "    \"taxamount\",\n",
    "    \"taxvaluedollarcnt\",\n",
    "    \"landtaxvaluedollarcnt\",\n",
    "    \"structuretaxvaluedollarcnt\",\n",
    "    \"finishedsquarefeet12\",\n",
    "    \"calculatedfinishedsquarefeet\",\n",
    "    \"fullbathcnt\",\n",
    "    \"lotsizesquarefeet\",\n",
    "    \"heatingorsystemtypeid\"\n",
    "]\n",
    "\n",
    "\n",
    "for col in cols:\n",
    "    median = train[col].median()\n",
    "    train[col].fillna(median, inplace=True)\n",
    "    validate[col].fillna(median, inplace=True)\n",
    "    test[col].fillna(median, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.isnull().sum().sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical/Discrete columns to use mode to replace nulls\n",
    "\n",
    "cols = [\n",
    "    \"buildingqualitytypeid\",\n",
    "    \"regionidcity\",\n",
    "    \"regionidzip\",\n",
    "    \"yearbuilt\",\n",
    "    \"regionidcity\",\n",
    "    \"censustractandblock\"\n",
    "]\n",
    "\n",
    "for col in cols:\n",
    "    mode = int(train[col].mode()) # I had some friction when this returned a float (and there were no decimals anyways)\n",
    "    train[col].fillna(value=mode, inplace=True)\n",
    "    validate[col].fillna(value=mode, inplace=True)\n",
    "    test[col].fillna(value=mode, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.isnull().sum().sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols3 = [\n",
    "    \"unitcnt\"\n",
    "]\n",
    "\n",
    "for col in cols3:\n",
    "    train[col].fillna(value=1, inplace=True)\n",
    "    validate[col].fillna(value=1, inplace=True)\n",
    "    test[col].fillna(value=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.isnull().sum().sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imputing_missing_values(train, validate, test):\n",
    "    \n",
    "    \n",
    "    cols = [\n",
    "    \"structuretaxvaluedollarcnt\",\n",
    "    \"taxamount\",\n",
    "    \"taxvaluedollarcnt\",\n",
    "    \"landtaxvaluedollarcnt\",\n",
    "    \"structuretaxvaluedollarcnt\",\n",
    "    \"finishedsquarefeet12\",\n",
    "    \"calculatedfinishedsquarefeet\",\n",
    "    \"fullbathcnt\",\n",
    "    \"lotsizesquarefeet\",\n",
    "    \"heatingorsystemtypeid\"\n",
    "    ]\n",
    "\n",
    "\n",
    "    for col in cols:\n",
    "        median = train[col].median()\n",
    "        train[col].fillna(median, inplace=True)\n",
    "        validate[col].fillna(median, inplace=True)\n",
    "        test[col].fillna(median, inplace=True)\n",
    "\n",
    "\n",
    "    # Categorical/Discrete columns to use mode to replace nulls\n",
    "\n",
    "    cols2 = [\n",
    "        \"buildingqualitytypeid\",\n",
    "        \"regionidcity\",\n",
    "        \"regionidzip\",\n",
    "        \"yearbuilt\",\n",
    "        \"regionidcity\",\n",
    "        \"censustractandblock\"\n",
    "    ]\n",
    "\n",
    "    for col in cols2:\n",
    "        mode = int(train[col].mode()) # I had some friction when this returned a float (and there were no decimals anyways)\n",
    "        train[col].fillna(value=mode, inplace=True)\n",
    "        validate[col].fillna(value=mode, inplace=True)\n",
    "        test[col].fillna(value=mode, inplace=True)\n",
    "    \n",
    "    # Taking care of unit count.\n",
    "    cols3 = [\n",
    "        \"unitcnt\"\n",
    "    ]\n",
    "\n",
    "    for col in cols3:\n",
    "        train[col].fillna(value=1, inplace=True)\n",
    "        validate[col].fillna(value=1, inplace=True)\n",
    "        test[col].fillna(value=1, inplace=True)\n",
    "    \n",
    "        \n",
    "    return train, validate, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imputing_missing_values(df):\n",
    "    \n",
    "    # First, inputing the median values:\n",
    "    df.regionidcity = df.regionidcity.fillna(df.regionidcity.median())\n",
    "    df.regionidzip = df.regionidzip.fillna(df.regionidzip.median())\n",
    "    df.yearbuilt = df.yearbuilt.fillna(df.yearbuilt.median())\n",
    "    df.censustractandblock = df.censustractandblock.fillna(df.censustractandblock.median())\n",
    "    \n",
    "    # Now using the mean to input the rest of the missing values:\n",
    "    df.lotsizesquarefeet = df.lotsizesquarefeet.fillna(df.lotsizesquarefeet.mean())\n",
    "    df.finishedsquarefeet12 = df.finishedsquarefeet12.fillna(df.finishedsquarefeet12.mean())\n",
    "    df.calculatedbathnbr = df.calculatedbathnbr.fillna(df.calculatedbathnbr.mean())\n",
    "    df.fullbathcnt = df.fullbathcnt.fillna(round(df.fullbathcnt.mean(),0))\n",
    "    df.calculatedfinishedsquarefeet = df.calculatedfinishedsquarefeet.fillna(round(df.calculatedfinishedsquarefeet.mean(),0))\n",
    "    df.structuretaxvaluedollarcnt = df.structuretaxvaluedollarcnt.fillna(round(df.structuretaxvaluedollarcnt.mean(),0))\n",
    "    df.taxamount = df.taxamount.fillna(round(df.taxamount.mean(),0))\n",
    "    df.landtaxvaluedollarcnt = df.landtaxvaluedollarcnt.fillna(round(df.landtaxvaluedollarcnt.mean(),0))\n",
    "    df.taxvaluedollarcnt = df.taxvaluedollarcnt.fillna(round(df.taxvaluedollarcnt.mean(),0))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now imputing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I should actually be imputing values AFTER I split the data.\n",
    "\n",
    "median = df.regionidcity.median()\n",
    "df.regionidcity = df.regionidcity.fillna(df.regionidcity.median())\n",
    "df.regionidzip = df.regionidzip.fillna(df.regionidzip.median())\n",
    "df.censustractandblock = df.censustractandblock.fillna(df.censustractandblock.median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum().sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now for the mean:\n",
    "\n",
    "df.lotsizesquarefeet = df.lotsizesquarefeet.fillna(df.lotsizesquarefeet.mean())\n",
    "df.finishedsquarefeet12 = df.finishedsquarefeet12.fillna(df.finishedsquarefeet12.mean())\n",
    "df.calculatedbathnbr = df.calculatedbathnbr.fillna(df.calculatedbathnbr.mean())\n",
    "df.fullbathcnt = df.fullbathcnt.fillna(round(df.fullbathcnt.mean(),0))\n",
    "df.calculatedfinishedsquarefeet = df.calculatedfinishedsquarefeet.fillna(round(df.calculatedfinishedsquarefeet.mean(),0))\n",
    "df.structuretaxvaluedollarcnt = df.structuretaxvaluedollarcnt.fillna(round(df.structuretaxvaluedollarcnt.mean(),0))\n",
    "df.taxamount = df.taxamount.fillna(round(df.taxamount.mean(),0))\n",
    "df.landtaxvaluedollarcnt = df.landtaxvaluedollarcnt.fillna(round(df.landtaxvaluedollarcnt.mean(),0))\n",
    "df.taxvaluedollarcnt = df.taxvaluedollarcnt.fillna(round(df.taxvaluedollarcnt.mean(),0))\n",
    "\n",
    "df.isna().sum().sort_values(ascending = False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All Missing values removed or imputed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols3 = [\"heatingorsystemdesc\"]\n",
    "\n",
    "for col in cols3:\n",
    "#     median = train[col].median()\n",
    "    train[col].fillna(\"None\", inplace = True)\n",
    "    validate[col].fillna(\"None\", inplace = True)\n",
    "    test[col].fillna(\"None\", inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mall Customers\n",
    "\n",
    "**notebook**\n",
    "- Acquire data from mall_customers.customers in mysql database.\n",
    "- Summarize data (include distributions and descriptive statistics).\n",
    "- Detect outliers using IQR.\n",
    "- Split data (train-test-split).\n",
    "- Encode categorical columns using a one hot encoder.\n",
    "- Handles missing values.\n",
    "- Scaling\n",
    "\n",
    "wrangle_mall.py\n",
    "- Acquire data from mall_customers.customers in mysql database.\n",
    "- Split the data\n",
    "- One-hot-encoding\n",
    "- Missing values\n",
    "- Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mall_wrangle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mall = mall_wrangle.get_mall_data()\n",
    "df_mall.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarizing the data:\n",
    "\n",
    "df_mall.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'The shape of this dataframe is {df_mall.shape}')\n",
    "df_mall.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detect outliers using IQR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the distribution of the columns\n",
    "\n",
    "df_mall.hist(figsize=(24, 10), bins = 20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mall.spending_score.hist(bins = 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q1 = df_mall.spending_score.quantile(.25)\n",
    "q3 = df_mall.spending_score.quantile(.75)\n",
    "\n",
    "q1, q3\n",
    "iqr = q3 - q1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 1.5\n",
    "upper_bound = q3 + (k * iqr)\n",
    "upper_bound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mall[df_mall.spending_score > upper_bound]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lower_bound = q1 - (k * iqr)\n",
    "lower_bound\n",
    "df_mall[df_mall.spending_score < lower_bound]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does not appear to be any observations outside of the upper or lower bounds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_upper_outliers(s, k):\n",
    "    '''\n",
    "    Given a series and a cutoff value, k, returns the upper outliers for the\n",
    "    series.\n",
    "\n",
    "    The values returned will be either 0 (if the point is not an outlier), or a\n",
    "    number that indicates how far away from the upper bound the observation is.\n",
    "    '''\n",
    "    q1, q3 = s.quantile([.25, .75])\n",
    "    iqr = q3 - q1\n",
    "    upper_bound = q3 + k * iqr\n",
    "    return s.apply(lambda x: max([x - upper_bound, 0]))\n",
    "\n",
    "def add_upper_outlier_columns(df, k):\n",
    "    '''\n",
    "    Add a column with the suffix _outliers for all the numeric columns\n",
    "    in the given dataframe.\n",
    "    '''\n",
    "    # outlier_cols = {col + '_outliers': get_upper_outliers(df[col], k)\n",
    "    #                 for col in df.select_dtypes('number')}\n",
    "    # return df.assign(**outlier_cols)\n",
    "\n",
    "    for col in df.select_dtypes('number'):\n",
    "        df[col + '_outliers'] = get_upper_outliers(df[col], k)\n",
    "\n",
    "    return df\n",
    "\n",
    "add_upper_outlier_columns(df_mall, k=1.5)\n",
    "\n",
    "df_mall.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_mall_data_test(df):\n",
    "    # df = get_mall_data()\n",
    "    # Splitting my data based on the target variable of tenure:\n",
    "    train_validate, test = train_test_split(df, test_size=.15, random_state=123)\n",
    "    \n",
    "    # Splitting the train_validate set into the separate train and validate datasets.\n",
    "    train, validate = train_test_split(train_validate, test_size=.20, random_state=123)\n",
    "    \n",
    "    # Printing the shape of each dataframe:\n",
    "    print(f'Shape of train df: {train.shape}')\n",
    "    print(f'Shape of validate df: {validate.shape}')\n",
    "    print(f'Shape of test df: {test.shape}')\n",
    "    return train, validate, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I built a function previously that split the mall data:\n",
    "\n",
    "train, validate, test = split_mall_data(df_mall)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode categorical columns using a one hot encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on what I've read, .get_dumbies is a one-hot-encoder. It's only a true dummy encoder if I select to drop the first column in the function.\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "ohe = OneHotEncoder(sparse=False, categories='gender')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Couldn't get this to work...\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "ohe = OneHotEncoder(sparse=False, categories='gender')\n",
    "# train_matrix = ohe.fit_transform(train[['col']])\n",
    "# validate_matrix = ohe.transform(validate[['col']])\n",
    "# test_matrix = ohe.transform(test[['col']])\n",
    "# train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['is_male'] = pd.get_dummies(train.gender, drop_first = True) # Leaving the drop_first() argument set to True means that both columns are return.\n",
    "train.head()\n",
    "# One for female, one for male. Note that returns an \"extra\" column that isn't strictly necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validate['is_male'] = pd.get_dummies(validate.gender, drop_first = True)\n",
    "test['is_male'] = pd.get_dummies(test.gender, drop_first = True)\n",
    "\n",
    "validate.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If I run the OneHotEncoder, I should end up with basically the same result.\n",
    "\n",
    "enc = preprocessing.OneHotEncoder()\n",
    "\n",
    "enc.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onehotlabels = enc.transform(train).toarray(1)\n",
    "onehotlabels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onehotlabels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I couldn't get the OneHotEncoder to work. Still, I was able to accomplish the work by using the .get_dummies, and simply not setting the drop_first argument = False."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Handles missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# There do not appear to be any misisng values, but I'll add some code in there...\n",
    "\n",
    "train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# For filling in any nulls\n",
    "train.spending_score.fillna(train.spending_score.mean(), inplace = True)\n",
    "train.age.fillna(train.age.mean(), inplace = True)\n",
    "train.annual_income.fillna(train.annual_income.median(), inplace = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
